# âš ï¸ [ë³€ê²½ ë‚´ì—­]
# ê¸°ì¡´: EXAONE-3.5-7.8B-Instruct ì‚¬ìš©
# ë³€ê²½: EXAONE-3.5-2.4B-Instruct ì‚¬ìš© (ê²½ëŸ‰ ëª¨ë¸ë¡œ ë©”ëª¨ë¦¬ ë° ì†ë„ ìµœì í™” ëª©ì )
# ë³€ê²½ì¼: 2025-06-10

import os
import json
import pandas as pd
from rdflib import Graph
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from sentence_transformers import CrossEncoder
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS as LangchainFAISS
from langchain.schema import Document
import faiss
import torch
import re
import unicodedata
from konlpy.tag import Okt
import markdown
import logging

# ë¡œê¹… ê¸°ë³¸ ì„¤ì •
logging.basicConfig(
    level=logging.INFO,  # ë¡œê·¸ ë ˆë²¨ (DEBUG, INFO, WARNING, ERROR, CRITICAL ì¤‘ ì„ íƒ ê°€ëŠ¥)
    format="%(asctime)s [%(levelname)s] %(message)s",  # ë¡œê·¸ ì¶œë ¥ í˜•ì‹
    datefmt="%Y-%m-%d %H:%M:%S"
)

# 1. ê²½ë¡œ ì„¤ì •
base_path = "/mnt/e/chatbot_project_data/law_chatbot_dataset"
qa_data_root = os.path.join(base_path, "Law_Regulations_Text_Analysis_Data")
folders = {
    "terms_json": os.path.join(base_path, "law_knowledge_base", "ë²•ë ¹ìš©ì–´"),
    "ontology_nt": os.path.join(base_path, "law_knowledge_base", "ë²•ë¥  ë°ì´í„°"),
    "feedback_log": os.path.join(base_path, "feedback_log.csv")
}

# 2. JSON / RDF ë¡œë”© í•¨ìˆ˜
def load_json_files(folder_path):
    data = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".json"):
                try:
                    with open(os.path.join(root, file), encoding="utf-8") as f:
                        data.append(json.load(f))
                except Exception as e:
                    print(f"[âŒ JSON ë¡œë”© ì‹¤íŒ¨] {file} â†’ {e}")
    return data

def load_rdf_files(folder_path):
    graphs = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".nt") or file.endswith(".owl"):
                g = Graph()
                try:
                    g.parse(os.path.join(root, file), format="nt" if file.endswith(".nt") else "xml")
                    graphs.append(g)
                except:
                    pass
    return graphs

def extract_triples(graphs):
    triples = []
    for g in graphs:
        for s, p, o in g:
            triples.append({"subject": str(s), "predicate": str(p), "object": str(o)})
    return pd.DataFrame(triples)

# 3. ì¡°í•­ QA ì¶”ì¶œ ë° í…ìŠ¤íŠ¸í™”
def extract_qa_from_clause_json(folder_path):
    qa_pairs = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".json"):
                try:
                    with open(os.path.join(root, file), encoding="utf-8") as f:
                        data = json.load(f)
                        if isinstance(data, dict):
                            label = str(data.get("unfavorableProvision") or "").strip()
                            if label:
                                qa_pairs.append({
                                    "question": "ì´ ì¡°í•­ì´ ì†Œë¹„ìì—ê²Œ ë¶ˆë¦¬í•œê°€ìš”?",
                                    "answer": label
                                })
                except:
                    pass
    return pd.DataFrame(qa_pairs)

def map_answer_label(label):
    if str(label) == "1":
        return "ì´ ì¡°í•­ì€ ì†Œë¹„ìì—ê²Œ ë¶ˆë¦¬í•œ ì¡°í•­ì…ë‹ˆë‹¤."
    elif str(label) == "2":
        return "ì´ ì¡°í•­ì€ ì†Œë¹„ìì—ê²Œ ìœ ë¦¬í•œ ì¡°í•­ì…ë‹ˆë‹¤."
    else:
        return "ì´ ì¡°í•­ì˜ ìœ ë¶ˆë¦¬ëŠ” ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

# law_qa_df = pd.read_pickle("/mnt/e/chatbot_project_data/law_chatbot_dataset/law_qa_df.pkl")

# 3-1. âœ… law_qa_df.pkl Fallback ë¡œì§ (ìë™ ìƒì„± ë° ì €ì¥ ê°€ëŠ¥)
# - [ìë™í™”] law_qa_df.pkl íŒŒì¼ì´ ì—†ìœ¼ë©´ JSONì—ì„œ QA ì¶”ì¶œ í›„ ì €ì¥ ì˜µì…˜ ì œê³µ
# - íŒŒì¼ì´ ìˆìœ¼ë©´ ë°”ë¡œ ë¡œë“œí•˜ì—¬ ì†ë„ í–¥ìƒ
# - íŒŒì¼ì´ ì—†ì„ ê²½ìš° fallbackìœ¼ë¡œ JSON íŒŒì‹± í›„ ì €ì¥ ì—¬ë¶€ ì„ íƒ

# ì„¤ì •ê°’: ìë™ ì €ì¥ ì—¬ë¶€
AUTO_SAVE = True  # Trueë©´ ìë™ ì €ì¥, Falseë©´ ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë´„

# law_qa_df.pkl ê²½ë¡œ
qa_pickle_path = os.path.join(base_path, "law_qa_df.pkl")

if os.path.exists(qa_pickle_path):
    print("âœ… law_qa_df.pkl ë¡œë“œ ì¤‘...")
    law_qa_df = pd.read_pickle(qa_pickle_path)
else:
    print("âš ï¸ law_qa_df.pkl ì—†ìŒ â†’ JSONì—ì„œ ì¶”ì¶œí•©ë‹ˆë‹¤.")
    law_qa_df = extract_qa_from_clause_json(qa_data_root)
    law_qa_df["answer"] = law_qa_df["answer"].apply(map_answer_label)

    if AUTO_SAVE:
        law_qa_df.to_pickle(qa_pickle_path)
        print("âœ… ìë™ ì €ì¥ ì™„ë£Œ:", qa_pickle_path)
    else:
        save = input("â“ ì¶”ì¶œëœ QAë¥¼ pklë¡œ ì €ì¥í• ê¹Œìš”? (y/n): ")
        if save.lower() == "y":
            law_qa_df.to_pickle(qa_pickle_path)
            print("âœ… ì €ì¥ ì™„ë£Œ:", qa_pickle_path)


# 4. LangChain ê¸°ë°˜ ë¬¸ì„œí™” ë° ë²¡í„° DB ìƒì„±
documents = [
    Document(page_content=f"{row['question']}\n{row['answer']}")
    for _, row in law_qa_df.iterrows()
]

embedding_model = HuggingFaceEmbeddings(model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS")
vectorstore = LangchainFAISS.from_documents(documents, embedding_model)

# 5. ì¬ë­ì»¤
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

# 6. RDF/ìš©ì–´ ë°ì´í„°
terms_dict = {
    item["ìš©ì–´"]: item["ì •ì˜"]
    for item in load_json_files(folders["terms_json"])
    if isinstance(item, dict) and "ìš©ì–´" in item and "ì •ì˜" in item
}
law_triple_df = extract_triples(load_rdf_files(folders["ontology_nt"]))

# 7. EXAONE ëª¨ë¸ ë¡œë”©
model_path = "LGAI-EXAONE/EXAONE-3.5-2.4B-instruct"
bnb_config = BitsAndBytesConfig(load_in_8bit=True,llm_int8_enable_fp32_cpu_offload=True)

# ì „ì—­ì—ì„œ ë‹¨ 1íšŒ ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    trust_remote_code=True,
    quantization_config=bnb_config,
    device_map="auto"
)

# 7-1. ëª¨ë¸ ì‘ë‹µ í•¨ìˆ˜
def ask_exaone(prompt):
    # ì…ë ¥ í† í°í™” ë° ëª¨ë¸ì— ì „ë‹¬
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # í…ìŠ¤íŠ¸ ìƒì„±
    output = model.generate(
        **inputs,
        max_new_tokens=1024,
        do_sample=False,  # ğŸ”¹ ìƒ˜í”Œë§ ë¹„í™œì„±í™” â†’ í•­ìƒ ê°™ì€ ì…ë ¥ì— ëŒ€í•´ ë™ì¼í•œ ì¶œë ¥ (ë¹ ë¥´ê³  ì¼ê´€ëœ ì‘ë‹µ)
        repetition_penalty=1.1,  # ğŸ”¹ ë°˜ë³µ ë°©ì§€ í˜ë„í‹° â†’ ê°™ì€ ë‹¨ì–´/ë¬¸ì¥ì´ ë°˜ë³µë˜ì§€ ì•Šë„ë¡ í•¨
        early_stopping=True,  # ğŸ”¹ EOS í† í°ì´ ë‚˜ì˜¤ë©´ ì¦‰ì‹œ ìƒì„± ì¤‘ë‹¨ (ë¶ˆí•„ìš”í•˜ê²Œ ê¸´ ì‘ë‹µ ë°©ì§€)
        eos_token_id=tokenizer.eos_token_id,  # ğŸ”¹ ë¬¸ì¥ ì¢…ë£Œ í† í° (ì´ í† í°ì´ ìƒì„±ë˜ë©´ ì¢…ë£Œ)
        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id  # ğŸ”¹ íŒ¨ë”© í† í° ì„¤ì • (ì—†ìœ¼ë©´ eos_tokenìœ¼ë¡œ ëŒ€ì²´)
    )
    
    # ì‘ë‹µ ë””ì½”ë”© ë° ë§ˆí¬ë‹¤ìš´ ë³€í™˜ (í‘œ ë Œë”ë§ í¬í•¨)
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    cleaned = response.replace(prompt, "").strip()
    return markdown.markdown(cleaned, extensions=['markdown.extensions.tables'])

# 8. ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ (LangChain + ì¬ë­ì»¤)
def retrieve_similar_qa(user_question, top_k=5):
    retrieved = vectorstore.similarity_search(user_question, k=top_k)
    qa_pairs = []
    for doc in retrieved:
        parts = doc.page_content.split("\n", 1)
        if len(parts) == 2:
            qa_pairs.append((parts[0], parts[1]))
    if not qa_pairs:
        return []
    scores = reranker.predict([[user_question, q] for q, _ in qa_pairs])
    return [pair for pair, _ in sorted(zip(qa_pairs, scores), key=lambda x: x[1], reverse=True)]

# 8-1. ì™¸ë¶€ ëª¨ë“ˆì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ wrapper í•¨ìˆ˜ ì œê³µ
def search_similar_questions(user_question, top_k=5):
    logging.info(f"ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ ì‹œì‘: '{user_question}'")
    return retrieve_similar_qa(user_question, top_k=top_k)

# 9. ë³´ì¡° ì§€ì‹ ì‘ë‹µ
def lookup_legal_term_definition(user_input):
    for term in terms_dict:
        if term in user_input:
            return f"'{term}'ì˜ ì •ì˜: {terms_dict[term]}"
    return None

def search_rdf_triple(user_input):
    results = []
    for _, row in law_triple_df.iterrows():
        if row["subject"] in user_input or row["object"] in user_input:
            results.append(f"{row['subject']} -[{row['predicate']}]-> {row['object']}")
        if len(results) >= 3:
            break
    return "\n".join(results) if results else None

# 10. ì „ì²˜ë¦¬ + í‚¤ì›Œë“œ ì¶”ì¶œ
def clean_question(text: str) -> str:
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r"[\t\n\r]+", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def extract_keywords_morph(text: str, top_k: int = 5) -> list[str]:
    okt = Okt()
    words = [w for w, t in okt.pos(text) if t in ("Noun", "Alpha") and len(w) > 1]
    freq = {}
    for word in words:
        freq[word] = freq.get(word, 0) + 1
    sorted_words = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [w for w, _ in sorted_words[:top_k]]

# 11. ìµœì¢… í†µí•© ì±—ë´‡ í•¨ìˆ˜
def smart_legal_chat(user_input):
    term_def = lookup_legal_term_definition(user_input)
    if term_def:
        return term_def

    rdf_info = search_rdf_triple(user_input)
    if rdf_info:
        return f"ğŸ” RDF ì •ë³´:\n{rdf_info}"

    cleaned = clean_question(user_input)
    keywords = extract_keywords_morph(cleaned)
    top_qas = retrieve_similar_qa(cleaned, top_k=5)

    if top_qas:
        q, a = top_qas[0]
        prompt = f"ì‚¬ìš©ì ì§ˆë¬¸: {cleaned}\n\ní‚¤ì›Œë“œ: {', '.join(keywords)}\n\nì°¸ê³  ì§ˆë¬¸: {q}\n\nì°¸ê³  ë‹µë³€: {a}\n\nì´ ë‚´ìš©ì„ ì°¸ê³ í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    else:
        prompt = f"{cleaned}ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."

    return ask_exaone(prompt)

# 12. í”¼ë“œë°± ì €ì¥
def save_feedback(user_question, model_answer, user_feedback):
    log = pd.DataFrame([{
        "question": user_question,
        "model_answer": model_answer,
        "feedback": user_feedback
    }])
    if os.path.exists(folders["feedback_log"]):
        existing = pd.read_csv(folders["feedback_log"])
        pd.concat([existing, log], ignore_index=True).to_csv(folders["feedback_log"], index=False)
    else:
        log.to_csv(folders["feedback_log"], index=False)
