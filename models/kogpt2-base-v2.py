import os
import json
import pandas as pd
from rdflib import Graph
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import CrossEncoder
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS as LangchainFAISS
from langchain.schema import Document
import torch
import re
import unicodedata
from konlpy.tag import Okt
import markdown
import logging
import time
from functools import lru_cache

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# 1. ê²½ë¡œ ì„¤ì •
base_path = "/mnt/e/chatbot_project_data/law_chatbot_dataset"
qa_data_root = os.path.join(base_path, "Law_Regulations_Text_Analysis_Data")
folders = {
    "terms_json": os.path.join(base_path, "law_knowledge_base", "ë²•ë ¹ìš©ì–´"),
    "ontology_nt": os.path.join(base_path, "law_knowledge_base", "ë²•ë¥  ë°ì´í„°"),
    "feedback_log": os.path.join(base_path, "feedback_log.csv")
}

# 2. JSON / RDF ë¡œë”©

def load_json_files(folder_path):
    data = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".json"):
                try:
                    with open(os.path.join(root, file), encoding="utf-8") as f:
                        data.append(json.load(f))
                except Exception as e:
                    print(f"[âŒ JSON ë¡œë”© ì‹¤íŒ¨] {file} â†’ {e}")
    return data

def load_rdf_files(folder_path):
    graphs = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".nt") or file.endswith(".owl"):
                g = Graph()
                try:
                    g.parse(os.path.join(root, file), format="nt" if file.endswith(".nt") else "xml")
                    graphs.append(g)
                except:
                    pass
    return graphs

def extract_triples(graphs):
    triples = []
    for g in graphs:
        for s, p, o in g:
            triples.append({"subject": str(s), "predicate": str(p), "object": str(o)})
    return pd.DataFrame(triples)

# 3. QA ë¡œë”©

def extract_qa_from_clause_json(folder_path):
    qa_pairs = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".json"):
                try:
                    with open(os.path.join(root, file), encoding="utf-8") as f:
                        data = json.load(f)
                        if isinstance(data, dict):
                            label = str(data.get("unfavorableProvision") or "").strip()
                            if label:
                                qa_pairs.append({
                                    "question": "ì´ ì¡°í•­ì´ ì†Œë¹„ìì—ê²Œ ë¶ˆë¦¬í•œê°€ìš”?",
                                    "answer": label
                                })
                except:
                    pass
    return pd.DataFrame(qa_pairs)

def map_answer_label(label):
    if str(label) == "1":
        return "ì´ ì¡°í•­ì€ ì†Œë¹„ìì—ê²Œ ë¶ˆë¦¬í•œ ì¡°í•­ì…ë‹ˆë‹¤."
    elif str(label) == "2":
        return "ì´ ì¡°í•­ì€ ì†Œë¹„ìì—ê²Œ ìœ ë¦¬í•œ ì¡°í•­ì…ë‹ˆë‹¤."
    else:
        return "ì´ ì¡°í•­ì˜ ìœ ë¶ˆë¦¬ëŠ” ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

qa_pickle_path = os.path.join(base_path, "law_qa_df.pkl")
if os.path.exists(qa_pickle_path):
    law_qa_df = pd.read_pickle(qa_pickle_path)
else:
    law_qa_df = extract_qa_from_clause_json(qa_data_root)
    law_qa_df["answer"] = law_qa_df["answer"].apply(map_answer_label)
    law_qa_df.to_pickle(qa_pickle_path)

# 4. ë²¡í„° DB ë° ì„ë² ë”©

documents = [Document(page_content=f"{row['question']}\n{row['answer']}") for _, row in law_qa_df.iterrows()]
embedding_model = HuggingFaceEmbeddings(model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS")
vectorstore = LangchainFAISS.from_documents(documents, embedding_model)
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

# 5. ë³´ì¡° ì§€ì‹ ë¡œë”©

terms_dict = {
    item["ìš©ì–´"]: item["ì •ì˜"]
    for item in load_json_files(folders["terms_json"])
    if isinstance(item, dict) and "ìš©ì–´" in item and "ì •ì˜" in item
}
law_triple_df = extract_triples(load_rdf_files(folders["ontology_nt"]))

# 6. ëª¨ë¸ ë¡œë”©

model_path = "skt/kogpt2-base-v2"
global_tokenizer = None
global_model = None

def load_model_once():
    global global_tokenizer, global_model
    if global_tokenizer is None or global_model is None:
        global_tokenizer = AutoTokenizer.from_pretrained(model_path)
        global_model = AutoModelForCausalLM.from_pretrained(model_path)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        global_model.to(device)
        global_model.eval()
        print(f"âœ… KoGPT2 ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device})")

@lru_cache(maxsize=128)
def ask_kogpt2(prompt):
    load_model_once()
    prompt = prompt[:1500]
    inputs = global_tokenizer(prompt, return_tensors="pt", truncation=True).to(global_model.device)

    if "token_type_ids" in inputs:
        del inputs["token_type_ids"]

    start = time.time()
    with torch.no_grad():
        outputs = global_model.generate(
            **inputs,
            max_new_tokens=768,
            do_sample=True,
            temperature=0.85,
            top_p=0.95,
            repetition_penalty=1.1,
            eos_token_id=global_tokenizer.eos_token_id,
            pad_token_id=global_tokenizer.eos_token_id
        )
    end = time.time()

    result = global_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
    cleaned = result.replace(prompt.strip(), "").strip()

    print(f"â± KoGPT2 ì‘ë‹µ ì‹œê°„: {end - start:.2f}ì´ˆ")
    return markdown.markdown(cleaned if len(cleaned) >= 10 else "âš ï¸ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.", extensions=["markdown.extensions.tables"])

# 7. ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰

def retrieve_similar_qa(user_question, top_k=5):
    retrieved = vectorstore.similarity_search(user_question, k=top_k)
    qa_pairs = []
    for doc in retrieved:
        parts = doc.page_content.split("\n", 1)
        if len(parts) == 2:
            qa_pairs.append((parts[0], parts[1]))
    if not qa_pairs:
        return []
    scores = reranker.predict([[user_question, q] for q, _ in qa_pairs])
    return [pair for pair, _ in sorted(zip(qa_pairs, scores), key=lambda x: x[1], reverse=True)]

def search_similar_questions(user_question, top_k=5):
    return retrieve_similar_qa(user_question, top_k=top_k)

# 8. ì „ì²˜ë¦¬ ë° í‚¤ì›Œë“œ ì¶”ì¶œ

def clean_question(text: str) -> str:
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r"[\t\n\r]+", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def extract_keywords_morph(text: str, top_k: int = 5) -> list[str]:
    okt = Okt()
    words = [w for w, t in okt.pos(text) if t in ("Noun", "Alpha") and len(w) > 1]
    freq = {}
    for word in words:
        freq[word] = freq.get(word, 0) + 1
    sorted_words = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [w for w, _ in sorted_words[:top_k]]

# 9. ë³´ì¡° ì§€ì‹ ì¡°íšŒ

def lookup_legal_term_definition(user_input):
    for term in terms_dict:
        if term in user_input:
            return f"'{term}'ì˜ ì •ì˜: {terms_dict[term]}"
    return None

def search_rdf_triple(user_input):
    results = []
    for _, row in law_triple_df.iterrows():
        if row["subject"] in user_input or row["object"] in user_input:
            results.append(f"{row['subject']} -[{row['predicate']}]-> {row['object']}")
        if len(results) >= 3:
            break
    return "\n".join(results) if results else None

# 10. ìµœì¢… ì‘ë‹µ í•¨ìˆ˜

def smart_legal_chat(user_input):
    if "ì°¸ê³  ì§ˆë¬¸" in user_input and "ì°¸ê³  ë‹µë³€" in user_input:
        match = re.search(r"ì‚¬ìš©ì ì§ˆë¬¸:\s*(.+?)\n", user_input)
        question = match.group(1).strip() if match else user_input.strip()
        simple_prompt = f"{question}\nì‚¬ê¸°ì£„ ì„±ë¦½ ìš”ê±´ì„ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜."
        return ask_kogpt2(simple_prompt)

    term_def = lookup_legal_term_definition(user_input)
    if term_def:
        return term_def

    rdf_info = search_rdf_triple(user_input)
    if rdf_info:
        return f"ğŸ” RDF ì •ë³´:\n{rdf_info}"

    cleaned = clean_question(user_input)
    keywords = extract_keywords_morph(cleaned)
    top_qas = retrieve_similar_qa(cleaned, top_k=5)

    if top_qas:
        q, a = top_qas[0]
        prompt = f"ì§ˆë¬¸: {cleaned}\nì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ìš”ê±´ë§Œ ì¡°ëª©ì¡°ëª© ì„¤ëª…í•´ì£¼ì„¸ìš”:\nQ: {q}\nA: {a}\në‹µë³€:"
    else:
        prompt = f"{cleaned}\nì‚¬ê¸°ì£„ ì„±ë¦½ ìš”ê±´ì„ ê°„ê²°íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."

    return ask_kogpt2(prompt)

# 11. í”¼ë“œë°± ì €ì¥

def save_feedback(user_question, model_answer, user_feedback):
    log = pd.DataFrame([{ "question": user_question, "model_answer": model_answer, "feedback": user_feedback }])
    if os.path.exists(folders["feedback_log"]):
        existing = pd.read_csv(folders["feedback_log"])
        pd.concat([existing, log], ignore_index=True).to_csv(folders["feedback_log"], index=False)
    else:
        log.to_csv(folders["feedback_log"], index=False)
