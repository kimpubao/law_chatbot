# âš ï¸ [ë³€ê²½ ë‚´ì—­]
# ê¸°ì¡´: EXAONE-3.5-7.8B-Instruct ì‚¬ìš©
# ë³€ê²½: EXAONE-3.5-2.4B-Instruct ì‚¬ìš© (ê²½ëŸ‰ ëª¨ë¸ë¡œ ë©”ëª¨ë¦¬ ë° ì†ë„ ìµœì í™” ëª©ì )
# ë³€ê²½ì¼: 2025-06-10

import os
import json
import pandas as pd
from rdflib import Graph
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from sentence_transformers import CrossEncoder
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS as LangchainFAISS
from langchain.schema import Document
import faiss
import torch
import re
import unicodedata
from konlpy.tag import Okt
import markdown
import logging
import time

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# 1. ê²½ë¡œ ì„¤ì •
base_path = "/mnt/e/chatbot_project_data/law_chatbot_dataset"
qa_data_root = os.path.join(base_path, "Law_Regulations_Text_Analysis_Data")
folders = {
    "terms_json": os.path.join(base_path, "law_knowledge_base", "ë²•ë ¹ìš©ì–´"),
    "ontology_nt": os.path.join(base_path, "law_knowledge_base", "ë²•ë¥  ë°ì´í„°"),
    "feedback_log": os.path.join(base_path, "feedback_log.csv")
}

# 2. JSON / RDF ë¡œë”© í•¨ìˆ˜
def load_json_files(folder_path):
    data = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".json"):
                try:
                    with open(os.path.join(root, file), encoding="utf-8") as f:
                        data.append(json.load(f))
                except Exception as e:
                    print(f"[âŒ JSON ë¡œë”© ì‹¤íŒ¨] {file} â†’ {e}")
    return data

def load_rdf_files(folder_path):
    graphs = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".nt") or file.endswith(".owl"):
                g = Graph()
                try:
                    g.parse(os.path.join(root, file), format="nt" if file.endswith(".nt") else "xml")
                    graphs.append(g)
                except:
                    pass
    return graphs

def extract_triples(graphs):
    triples = []
    for g in graphs:
        for s, p, o in g:
            triples.append({"subject": str(s), "predicate": str(p), "object": str(o)})
    return pd.DataFrame(triples)

# 3. QA ë¡œë”©
def extract_qa_from_clause_json(folder_path):
    qa_pairs = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".json"):
                try:
                    with open(os.path.join(root, file), encoding="utf-8") as f:
                        data = json.load(f)
                        if isinstance(data, dict):
                            label = str(data.get("unfavorableProvision") or "").strip()
                            if label:
                                qa_pairs.append({
                                    "question": "ì´ ì¡°í•­ì´ ì†Œë¹„ìì—ê²Œ ë¶ˆë¦¬í•œê°€ìš”?",
                                    "answer": label
                                })
                except:
                    pass
    return pd.DataFrame(qa_pairs)

def map_answer_label(label):
    if str(label) == "1":
        return "ì´ ì¡°í•­ì€ ì†Œë¹„ìì—ê²Œ ë¶ˆë¦¬í•œ ì¡°í•­ì…ë‹ˆë‹¤."
    elif str(label) == "2":
        return "ì´ ì¡°í•­ì€ ì†Œë¹„ìì—ê²Œ ìœ ë¦¬í•œ ì¡°í•­ì…ë‹ˆë‹¤."
    else:
        return "ì´ ì¡°í•­ì˜ ìœ ë¶ˆë¦¬ëŠ” ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

qa_pickle_path = os.path.join(base_path, "law_qa_df.pkl")
if os.path.exists(qa_pickle_path):
    print("âœ… law_qa_df.pkl ë¡œë“œ ì¤‘...")
    law_qa_df = pd.read_pickle(qa_pickle_path)
else:
    print("âš ï¸ law_qa_df.pkl ì—†ìŒ â†’ JSONì—ì„œ ì¶”ì¶œí•©ë‹ˆë‹¤.")
    law_qa_df = extract_qa_from_clause_json(qa_data_root)
    law_qa_df["answer"] = law_qa_df["answer"].apply(map_answer_label)
    law_qa_df.to_pickle(qa_pickle_path)
    print("âœ… ìë™ ì €ì¥ ì™„ë£Œ:", qa_pickle_path)

# 4. ë²¡í„° DB
documents = [Document(page_content=f"{row['question']}\n{row['answer']}") for _, row in law_qa_df.iterrows()]
embedding_model = HuggingFaceEmbeddings(model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS")
vectorstore = LangchainFAISS.from_documents(documents, embedding_model)

# 5. ì¬ë­ì»¤
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

# 6. ìš©ì–´ ë° ì‚¼ì¤‘í•­
terms_dict = {
    item["ìš©ì–´"]: item["ì •ì˜"]
    for item in load_json_files(folders["terms_json"])
    if isinstance(item, dict) and "ìš©ì–´" in item and "ì •ì˜" in item
}
law_triple_df = extract_triples(load_rdf_files(folders["ontology_nt"]))

# 7. EXAONE ëª¨ë¸ ë¡œë”© (4bit ìµœì í™” + GPU ê³ ì •)
model_path = "./EXAONE-3.5-2.4B-Instruct"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16
)

global_tokenizer = AutoTokenizer.from_pretrained(
    model_path, 
    trust_remote_code=True
)

global_model = AutoModelForCausalLM.from_pretrained(
    model_path,
    trust_remote_code=True,
    quantization_config=bnb_config,
    device_map="cuda"
)


def load_model_once():
    global global_tokenizer, global_model
    if global_tokenizer is None or global_model is None:
        print("ğŸ”„ EXAONE ëª¨ë¸ ë¡œì»¬ì—ì„œ ë¡œë”© ì¤‘...")
        global_tokenizer = AutoTokenizer.from_pretrained(
            model_path, trust_remote_code=False
        )
        global_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=False,
            quantization_config=bnb_config,
            device_map="cuda"
        )
        print("âœ… ëª¨ë¸ ë””ë°”ì´ìŠ¤:", global_model.device)
    return global_tokenizer, global_model


def ask_exaone(prompt):
    tokenizer, model = load_model_once()
    inputs = tokenizer(prompt[:1500], return_tensors="pt").to(model.device)

    start = time.time()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=768,
            do_sample=False,
            repetition_penalty=1.1,
            early_stopping=True,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id
        )
    end = time.time()
    print(f"â± ëª¨ë¸ ì‘ë‹µ ì‹œê°„: {end - start:.2f}ì´ˆ")

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return markdown.markdown(result.replace(prompt.strip(), "").strip(), extensions=['markdown.extensions.tables'])

# 8. ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰
def retrieve_similar_qa(user_question, top_k=5):
    retrieved = vectorstore.similarity_search(user_question, k=top_k)
    qa_pairs = []
    for doc in retrieved:
        parts = doc.page_content.split("\n", 1)
        if len(parts) == 2:
            qa_pairs.append((parts[0], parts[1]))
    if not qa_pairs:
        return []
    scores = reranker.predict([[user_question, q] for q, _ in qa_pairs])
    return [pair for pair, _ in sorted(zip(qa_pairs, scores), key=lambda x: x[1], reverse=True)]

# 9. ë³´ì¡° ì§€ì‹ ì‘ë‹µ
def lookup_legal_term_definition(user_input):
    for term in terms_dict:
        if term in user_input:
            return f"'{term}'ì˜ ì •ì˜: {terms_dict[term]}"
    return None

def search_rdf_triple(user_input):
    results = []
    for _, row in law_triple_df.iterrows():
        if row["subject"] in user_input or row["object"] in user_input:
            results.append(f"{row['subject']} -[{row['predicate']}]-> {row['object']}")
        if len(results) >= 3:
            break
    return "\n".join(results) if results else None

# 10. ì „ì²˜ë¦¬ ë° í‚¤ì›Œë“œ
def clean_question(text: str) -> str:
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r"[\t\n\r]+", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def extract_keywords_morph(text: str, top_k: int = 5) -> list[str]:
    okt = Okt()
    words = [w for w, t in okt.pos(text) if t in ("Noun", "Alpha") and len(w) > 1]
    freq = {}
    for word in words:
        freq[word] = freq.get(word, 0) + 1
    sorted_words = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [w for w, _ in sorted_words[:top_k]]

# 11. í†µí•© ì±—ë´‡ ì‘ë‹µ
def smart_legal_chat(user_input):
    term_def = lookup_legal_term_definition(user_input)
    if term_def:
        return term_def

    rdf_info = search_rdf_triple(user_input)
    if rdf_info:
        return f"ğŸ” RDF ì •ë³´:\n{rdf_info}"

    cleaned = clean_question(user_input)
    keywords = extract_keywords_morph(cleaned)
    top_qas = retrieve_similar_qa(cleaned, top_k=5)

    if top_qas:
        q, a = top_qas[0]
        prompt = f"""ì§ˆë¬¸: {cleaned}
ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:

Q: {q}
A: {a}

ë‹µë³€:"""
    else:
        prompt = f"ì§ˆë¬¸: {cleaned}\në‹µë³€:"

    return ask_exaone(prompt)

# 12. í”¼ë“œë°± ì €ì¥
def save_feedback(user_question, model_answer, user_feedback):
    log = pd.DataFrame([{
        "question": user_question,
        "model_answer": model_answer,
        "feedback": user_feedback
    }])
    if os.path.exists(folders["feedback_log"]):
        existing = pd.read_csv(folders["feedback_log"])
        pd.concat([existing, log], ignore_index=True).to_csv(folders["feedback_log"], index=False)
    else:
        log.to_csv(folders["feedback_log"], index=False)
