import os
import json
import pandas as pd
from rdflib import Graph
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import CrossEncoder
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS as LangchainFAISS
from langchain.schema import Document
import torch
import re
import unicodedata
from konlpy.tag import Okt
import markdown
import logging

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# 1. ê²½ë¡œ ì„¤ì •
base_path = "/mnt/e/chatbot_project_data/law_chatbot_dataset"
qa_data_root = os.path.join(base_path, "Law_Regulations_Text_Analysis_Data")
folders = {
    "terms_json": os.path.join(base_path, "law_knowledge_base", "ë²•ë ¹ìš©ì–´"),
    "ontology_nt": os.path.join(base_path, "law_knowledge_base", "ë²•ë¥  ë°ì´í„°"),
    "feedback_log": os.path.join(base_path, "feedback_log.csv")
}

# 2. ë°ì´í„° ë¡œë”©
def load_json_files(folder_path):
    data = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".json"):
                try:
                    with open(os.path.join(root, file), encoding="utf-8") as f:
                        data.append(json.load(f))
                except Exception as e:
                    print(f"[âŒ JSON ë¡œë”© ì‹¤íŒ¨] {file} â†’ {e}")
    return data

def load_rdf_files(folder_path):
    graphs = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".nt") or file.endswith(".owl"):
                g = Graph()
                try:
                    g.parse(os.path.join(root, file), format="nt" if file.endswith(".nt") else "xml")
                    graphs.append(g)
                except:
                    pass
    return graphs

def extract_triples(graphs):
    triples = []
    for g in graphs:
        for s, p, o in g:
            triples.append({"subject": str(s), "predicate": str(p), "object": str(o)})
    return pd.DataFrame(triples)

def extract_qa_from_clause_json(folder_path):
    qa_pairs = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".json"):
                try:
                    with open(os.path.join(root, file), encoding="utf-8") as f:
                        data = json.load(f)
                        if isinstance(data, dict):
                            label = str(data.get("unfavorableProvision") or "").strip()
                            if label:
                                qa_pairs.append({
                                    "question": "ì´ ì¡°í•­ì´ ì†Œë¹„ìì—ê²Œ ë¶ˆë¦¬í•œê°€ìš”?",
                                    "answer": label
                                })
                except:
                    pass
    return pd.DataFrame(qa_pairs)

def map_answer_label(label):
    if str(label) == "1":
        return "ì´ ì¡°í•­ì€ ì†Œë¹„ìì—ê²Œ ë¶ˆë¦¬í•œ ì¡°í•­ì…ë‹ˆë‹¤."
    elif str(label) == "2":
        return "ì´ ì¡°í•­ì€ ì†Œë¹„ìì—ê²Œ ìœ ë¦¬í•œ ì¡°í•­ì…ë‹ˆë‹¤."
    else:
        return "ì´ ì¡°í•­ì˜ ìœ ë¶ˆë¦¬ëŠ” ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

qa_pickle_path = os.path.join(base_path, "law_qa_df.pkl")
if os.path.exists(qa_pickle_path):
    law_qa_df = pd.read_pickle(qa_pickle_path)
else:
    law_qa_df = extract_qa_from_clause_json(qa_data_root)
    law_qa_df["answer"] = law_qa_df["answer"].apply(map_answer_label)
    law_qa_df.to_pickle(qa_pickle_path)

# 3. ë²¡í„° DB ë° ì¬ë­ì»¤
documents = [
    Document(page_content=f"{row['question']}\n{row['answer']}")
    for _, row in law_qa_df.iterrows()
]
embedding_model = HuggingFaceEmbeddings(model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS")
vectorstore = LangchainFAISS.from_documents(documents, embedding_model)
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

# 4. ìš©ì–´ì‚¬ì „ + RDF ì‚¼ì¤‘í•­
terms_dict = {
    item["ìš©ì–´"]: item["ì •ì˜"]
    for item in load_json_files(folders["terms_json"])
    if isinstance(item, dict) and "ìš©ì–´" in item and "ì •ì˜" in item
}
law_triple_df = extract_triples(load_rdf_files(folders["ontology_nt"]))

# 5. Polyglot ëª¨ë¸ ë¡œë”©
model_path = "EleutherAI/polyglot-ko-1.3b"
global_tokenizer = None
global_model = None

def load_model_once():
    global global_tokenizer, global_model
    if global_tokenizer is None or global_model is None:
        print("ğŸ”„ Polyglot ëª¨ë¸ ìµœì´ˆ ë¡œë”© ì¤‘...")
        global_tokenizer = AutoTokenizer.from_pretrained(model_path)
        global_model = AutoModelForCausalLM.from_pretrained(model_path)
        global_model.to("cuda" if torch.cuda.is_available() else "cpu")
        global_model.eval()
    return global_tokenizer, global_model

def ask_polyglot(prompt):
    tokenizer, model = load_model_once()
    inputs = tokenizer(prompt[:1500], return_tensors="pt", truncation=True)

    # âœ… token_type_ids ì œê±° (Polyglotì—ì„œ ë¯¸ì§€ì›)
    if "token_type_ids" in inputs:
        del inputs["token_type_ids"]

    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=768,
            do_sample=True,
            temperature=0.7,
            top_p=0.95,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id
        )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    cleaned = result.replace(prompt, "").strip()

    if not cleaned or len(cleaned) < 10:
        return "âš ï¸ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    return markdown.markdown(cleaned, extensions=["markdown.extensions.tables"])

# 6. ê²€ìƒ‰ í•¨ìˆ˜
def retrieve_similar_qa(user_question, top_k=5):
    retrieved = vectorstore.similarity_search(user_question, k=top_k)
    qa_pairs = []
    for doc in retrieved:
        parts = doc.page_content.split("\n", 1)
        if len(parts) == 2:
            qa_pairs.append((parts[0], parts[1]))
    if not qa_pairs:
        return []
    scores = reranker.predict([[user_question, q] for q, _ in qa_pairs])
    return [pair for pair, _ in sorted(zip(qa_pairs, scores), key=lambda x: x[1], reverse=True)]

# 7. ì „ì²˜ë¦¬ ë° í‚¤ì›Œë“œ
def clean_question(text: str) -> str:
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r"[\t\n\r]+", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def extract_keywords_morph(text: str, top_k: int = 5) -> list[str]:
    okt = Okt()
    words = [w for w, t in okt.pos(text) if t in ("Noun", "Alpha") and len(w) > 1]
    freq = {}
    for word in words:
        freq[word] = freq.get(word, 0) + 1
    sorted_words = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [w for w, _ in sorted_words[:top_k]]

# 8. ë²•ë ¹ ì •ì˜ + RDF ê²€ìƒ‰
def lookup_legal_term_definition(user_input):
    for term in terms_dict:
        if term in user_input:
            return f"'{term}'ì˜ ì •ì˜: {terms_dict[term]}"
    return None

def search_rdf_triple(user_input):
    results = []
    for _, row in law_triple_df.iterrows():
        if row["subject"] in user_input or row["object"] in user_input:
            results.append(f"{row['subject']} -[{row['predicate']}]-> {row['object']}")
        if len(results) >= 3:
            break
    return "\n".join(results) if results else None

# 9. ë©”ì¸ ì‘ë‹µ
def smart_legal_chat(user_input):
    term_def = lookup_legal_term_definition(user_input)
    if term_def:
        return term_def
    rdf_info = search_rdf_triple(user_input)
    if rdf_info:
        return f"ğŸ” RDF ì •ë³´:\n{rdf_info}"
    cleaned = clean_question(user_input)
    keywords = extract_keywords_morph(cleaned)
    top_qas = retrieve_similar_qa(cleaned, top_k=5)
    if top_qas:
        q, a = top_qas[0]
        prompt = f"ì‚¬ìš©ì ì§ˆë¬¸: {cleaned}\n\ní‚¤ì›Œë“œ: {', '.join(keywords)}\n\nì°¸ê³  ì§ˆë¬¸: {q}\n\nì°¸ê³  ë‹µë³€: {a}\n\nì´ ë‚´ìš©ì„ ì°¸ê³ í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    else:
        prompt = f"{cleaned}ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    return ask_polyglot(prompt)

# 10. ì™¸ë¶€ ëª¨ë“ˆ ì—°ë™ìš© ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ í•¨ìˆ˜
def search_similar_questions(user_question, top_k=5):
    return retrieve_similar_qa(user_question, top_k=top_k)

# 11. í”¼ë“œë°± ì €ì¥
def save_feedback(user_question, model_answer, user_feedback):
    log = pd.DataFrame([{
        "question": user_question,
        "model_answer": model_answer,
        "feedback": user_feedback
    }])
    if os.path.exists(folders["feedback_log"]):
        existing = pd.read_csv(folders["feedback_log"])
        pd.concat([existing, log], ignore_index=True).to_csv(folders["feedback_log"], index=False)
    else:
        log.to_csv(folders["feedback_log"], index=False)
